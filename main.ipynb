{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importing dataset. \n",
    "\"\"\"\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the root folder\n",
    "root_folder = \"Dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Running on: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Apple Silicon GPU\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # NVIDIA GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # if no GPU, then CPU\n",
    "\n",
    "print(\"Device Running on:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dataset \n",
    "df = pd.read_csv(root_folder+\"subset_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>file_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7230</td>\n",
       "      <td>subset_train_data/872586102a8e44738ca8fa97046c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3021</td>\n",
       "      <td>subset_train_data/5ba3fef1f8cb42cea0d3f43b98e6...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49745</td>\n",
       "      <td>subset_train_data/0f1ed6e90603411a89122c6de6e9...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60079</td>\n",
       "      <td>subset_train_data/f6422ac852aa4ab2a30ef9db7196...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3465</td>\n",
       "      <td>subset_train_data/0a7b016a42f14e1d9ba7fa98953c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          file_name  label\n",
       "0        7230  subset_train_data/872586102a8e44738ca8fa97046c...      1\n",
       "1        3021  subset_train_data/5ba3fef1f8cb42cea0d3f43b98e6...      0\n",
       "2       49745  subset_train_data/0f1ed6e90603411a89122c6de6e9...      0\n",
       "3       60079  subset_train_data/f6422ac852aa4ab2a30ef9db7196...      0\n",
       "4        3465  subset_train_data/0a7b016a42f14e1d9ba7fa98953c...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    6000\n",
      "1    4000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains 6000 real images and 4000 AI genertaed images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_image(df):\n",
    "    \"\"\"\n",
    "    Display a random image from the dataset with its label.\n",
    "    \"\"\"\n",
    "\n",
    "    # Choosing the random index\n",
    "    index = random.randint(0, len(df) - 1)\n",
    "\n",
    "    # Loading path and label\n",
    "    file_name = df.iloc[index]['file_name']\n",
    "    label = df.iloc[index]['label']\n",
    "\n",
    "    # Load the image \n",
    "    image = Image.open(root_folder + file_name).convert('RGB')\n",
    "\n",
    "    # Convert label\n",
    "    label_text = \"Real\" if label == 0 else \"AI-Generated\"\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Label: {label_text}\", fontsize=14, color=\"red\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_image(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Splitting dataset for training and testing\n",
    " - test_size = 20% \n",
    "\"\"\"\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    4815\n",
      "1    3185\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    1185\n",
      "1     815\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(val_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Frequency Domain - Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocessing and argumentation for the training and testing dataset. \n",
    "\"\"\"\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    \n",
    "    # Resize the image to 224 * 224  \n",
    "    transforms.Resize((224, 224)),  \n",
    "\n",
    "    # Randomly flip dataset  \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "\n",
    "    # Convert the image to PyTorch Tensor\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    # Normalizing the image, using imagenet mean and standard deviation\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "\n",
    "    # Resize the image to 224 * 224\n",
    "    transforms.Resize((224, 224)),\n",
    "\n",
    "    # Convert the image to PyTorch Tensor\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    # Normalizing the image, using imagenet mean and standard deviation\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_FD_Dataset(Dataset):\n",
    "    \"\"\"  \n",
    "        A Custom PyTorch Dataset to load images and labels from the dataset. \n",
    "        Converting the images to the frequency domain using FFT. \n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, image_dir, transform=None, file_name_col='file_name', label_col='label'):\n",
    "        \"\"\" \n",
    "            Arguments : \n",
    "                dataframe - imported dataset. \n",
    "                image_dir - directory of image dataset\n",
    "                transform - transformation for image\n",
    "                file_name_col - column name of the image path from dataset\n",
    "                label_col - column name of the labels (real or fake) from dataset\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.image_dir = image_dir\n",
    "        self.file_name_col = file_name_col\n",
    "        self.label_col = label_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" \n",
    "            Processing one sample using image and corresponing labels. \n",
    "        \"\"\"\n",
    "\n",
    "        # Retriving the image name and forming the image path. \n",
    "        img_file = os.path.basename(\n",
    "            self.dataframe.iloc[idx][self.file_name_col])\n",
    "        img_path = os.path.join(self.image_dir, img_file)\n",
    "\n",
    "        # Retriving the label. If not present assigning -1. \n",
    "        label = self.dataframe.iloc[idx][self.label_col] if self.label_col in self.dataframe.columns else -1\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = TF.to_tensor(image)\n",
    "\n",
    "        # Applying the transformation for the images before FFT converstion (Optional). \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Transforming image to FFT\n",
    "        freq_image = self._get_magnitude_spectrum(image)\n",
    "\n",
    "        return freq_image, label\n",
    "\n",
    "    def _get_magnitude_spectrum(self, img_tensor):\n",
    "        \"\"\"  \n",
    "        Converting image as frequency domain represtation using 2D FFT. \n",
    "        \"\"\"\n",
    "        freq = torch.fft.fft2(img_tensor)\n",
    "        freq_shift = torch.fft.fftshift(freq)\n",
    "        magnitude = torch.abs(freq_shift)\n",
    "        magnitude = torch.log1p(magnitude)  # log scale for stability\n",
    "        return magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up batch size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialization of training and testing dataset using custom dataset (CNN_FD_Dataset)\n",
    "\"\"\"\n",
    "\n",
    "# Training dataset initialization \n",
    "train_dataset = CNN_FD_Dataset(train_df,\n",
    "                              image_dir=root_folder + \"/subset_train_data\",\n",
    "                              transform=transform_train)\n",
    "\n",
    "# Testing dataset initialization\n",
    "val_dataset = CNN_FD_Dataset(val_df,\n",
    "                            image_dir=root_folder + \"/subset_train_data\",\n",
    "                            transform=transform_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Dataloader for training and validation \n",
    "\"\"\" \n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=False, \n",
    "                        num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size :  8000 samples\n",
      "Validation set size: 2000 samples\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set size : \", len(train_dataset), \"samples\")\n",
    "print(\"Validation set size:\", len(val_dataset), \"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_FD_Classifier(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=2, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=2, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=401408, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN_FD_Classifier(nn.Module):\n",
    "    \"\"\"  \n",
    "    CNN Model with Architecture : \n",
    "        -  3 convolution layer with ReLu activation function. \n",
    "        -  2 max pooling layers\n",
    "        -  2 fully connected layers with 0.5 dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=2):\n",
    "        super(CNN_FD_Classifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, \n",
    "                               out_channels=32, \n",
    "                               kernel_size=3, \n",
    "                               stride=1, \n",
    "                               padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, \n",
    "                               out_channels=64, \n",
    "                               kernel_size=3, \n",
    "                               stride=1, \n",
    "                               padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, \n",
    "                                 padding=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, \n",
    "                               kernel_size=3, \n",
    "                               stride=1, \n",
    "                               padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, \n",
    "                                  padding=2)\n",
    "\n",
    "        self.flatten_dim = 128 * 56 * 56  # assuming 224x224 input\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flatten_dim, 512)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool2(F.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "# Initialize the CNN model and move it to the designated device\n",
    "cnn_fd_model = CNN_FD_Classifier(num_classes=2).to(device)\n",
    "\n",
    "# Print the model architecture\n",
    "print(cnn_fd_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device, num_epochs):\n",
    "    \"\"\" \n",
    "    Trainer function to train the CNN_FD_Classifier PyTorch model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setting up the Pytorch model in training mode\n",
    "    model.train() \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        # Progress bar for each epochs\n",
    "        progress_bar = tqdm(\n",
    "            train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}', leave=False)\n",
    "        \n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # making gradients as zero before backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass outpt\n",
    "            outputs = model(images)\n",
    "            # predicted value and true value loss calculations\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backward propergation \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Update loss\n",
    "            running_loss += loss.item()\n",
    "            # Calculating the accuracy \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            progress_bar.set_postfix(\n",
    "                loss=loss.item(), acc=f\"{100 * correct / total:.2f}%\")\n",
    "        # Printing summary of epocs\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWIN Transformer - Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Model (Model 1 + Model 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
